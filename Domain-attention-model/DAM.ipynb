{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DAM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JHMrLBu9FQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Domain Attention Model\n",
        "\"\"\"\n",
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import numpy as np\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras import layers, callbacks\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import re\n",
        "\n",
        "from tqdm import tqdm\n",
        "from numpy.linalg import norm\n",
        "\n",
        "from keras import layers\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "\n",
        "# for reproducibility\n",
        "np.random.seed(2333)\n",
        "tf.set_random_seed(2333)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsbAEiZl9hdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_path = os.path.expanduser('')\n",
        "\n",
        "# domains\n",
        "DOMAINS = ('books', 'dvd', 'electronics', 'kitchen')\n",
        "\n",
        "\n",
        "    \"\"\"global data\"\"\"\n",
        "coef = 0.04  # domain loss coefficient in total loss\n",
        "\n",
        "# model params\n",
        "embed_dim = 300  # word embedding dimension\n",
        "rnn_dim = 300  # rnn state dimension\n",
        "hidden_dim = 100  # hidden state dimension\n",
        "embed_dropout = 0.2  # dropout rate for word embedding layer\n",
        "fc_dropout = 0.2  # dropout rate for fully connected layer\n",
        "batch_size = 64  # training batch size\n",
        "epochs = 100  # maximal training epochs\n",
        "activation = 'relu'  # activation function of fully connected layer\n",
        "optimizer = 'adadelta'  # optimizer method for training\n",
        "RNN = layers.LSTM  # RNN type, can be SimpleRNN/GRU/LSTM/...\n",
        "\n",
        "# train params\n",
        "lr_factor = 0.1  # reduce factor of learning rate for training plateau\n",
        "lr_patience = 3  # reduce learning rate when val loss has stopped improving\n",
        "stop_patience = 5  # stop training when val loss has stopped improving\n",
        "\n",
        "# data params\n",
        "glove_corpus = 6  # glove corpus size, can be 6B/42B/840B\n",
        "min_count = 1  # minimal word frequency cutoff\n",
        "max_words = None  # maximal words kept (None means all kept)\n",
        "n_words = None\n",
        "maxlen = None\n",
        "word2index = None\n",
        "wv_weights = None  # word vectors weights\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-PXSwEdwSt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Data Loader for MDSD Dataset\n",
        "\"\"\"\n",
        "\n",
        "def load_mdsd(domains, n_labeled=None):\n",
        "    texts = []\n",
        "    s_labels = []\n",
        "    d_labels = []\n",
        "    sentiments = ('positive', 'negative')\n",
        "    for d_id, d_name in enumerate(domains):\n",
        "        for s_id, s_name in zip((1, 0, -1), sentiments):\n",
        "            fpath = os.path.join('', '', d_name + '_' + s_name + '.review')\n",
        "            print(' - loading', d_name, s_name, end='')\n",
        "            count = 0\n",
        "            text = ''\n",
        "            in_review_text = False\n",
        "            with open(fpath, encoding='utf8', errors='ignore') as fr:\n",
        "                for line in fr:\n",
        "                    if '<review_text>' in line:\n",
        "                        text = ''\n",
        "                        in_review_text = True\n",
        "                        continue\n",
        "                    if '</review_text>' in line:\n",
        "                        in_review_text = False\n",
        "                        text = text.lower().replace('\\n', ' ').strip()\n",
        "                        text = re.sub(r'&[a-z]+;', '', text)\n",
        "                        text = re.sub(r'\\s+', ' ', text)\n",
        "                        texts.append(text)\n",
        "                        s_labels.append(s_id)\n",
        "                        d_labels.append(d_id)\n",
        "                        count += 1\n",
        "                    if in_review_text:\n",
        "                        text += line\n",
        "                    # labeled cutoff\n",
        "                    if (s_id >= 0) and n_labeled and (count == n_labeled):\n",
        "                        break\n",
        "            print(': %d texts' % count)\n",
        "    print('data loaded')\n",
        "    s_labels = np.asarray(s_labels, dtype='int')\n",
        "    d_labels = np.asarray(d_labels, dtype='int')\n",
        "    print(' - texts:', len(texts))\n",
        "    print(' - s_labels:', len(s_labels))\n",
        "    print(' - d_labels:', len(d_labels))\n",
        "\n",
        "    return texts, s_labels, d_labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT-z5KWR1Mt7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "6484a567-7226-4e7e-b613-32d31eef394f"
      },
      "source": [
        "\"\"\"\n",
        "Utils\n",
        "\"\"\"\n",
        "\n",
        "assert os.path.exists(glove_path)\n",
        "\n",
        "\n",
        "def load_glove(path=glove_path, embedding_dim=300, corpus_size=6, desired=None, verbose=False):\n",
        "    \"\"\"Load glove embeddings from original txt file\n",
        "    \"\"\"\n",
        "    if embedding_dim != 300:\n",
        "        assert embedding_dim in (50, 100, 200), 'embedding dim must be one of 50/100/200 if not 300'\n",
        "        fpath = os.path.join(path, 'glove.6B.{}d.txt'.format(embedding_dim))\n",
        "    else:\n",
        "        assert corpus_size in (6, 42, 840), 'corpus type must be one of 6B/42B/840B'\n",
        "        fpath = os.path.join(path, 'glove.{}B.300d.txt'.format(corpus_size))\n",
        "    word2vec = {}\n",
        "    print('loading glove from', fpath)\n",
        "    f = open(fpath, 'r', encoding='utf8', errors='ignore')\n",
        "    for line in tqdm(f, desc='glove') if verbose else f:\n",
        "        values = line.split()\n",
        "        word = values[0]  # the word\n",
        "        if not desired or word in desired:\n",
        "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "            word2vec[word] = coefs\n",
        "    f.close()\n",
        "    print('glove info: {} words, {} dims'.format(len(word2vec), embedding_dim))\n",
        "    return word2vec\n",
        "\n",
        "\n",
        "def get_embedding_mat(embeddings, word2index, embedding_dim, random_uniform_level=0.01, idx_from=2):\n",
        "    \"\"\"Use embeddings and word2index to get embedding-mat (for input layer)\n",
        "    idx_from=2, usually, 0 for <PAD>, 1 for <OOV>\n",
        "    \"\"\"\n",
        "    # embedding_mat = np.zeros((n_words, embedding_dim))\n",
        "    n_words = len(word2index)\n",
        "    for idx in range(0, idx_from):\n",
        "        if idx in word2index.values():\n",
        "            n_words -= 1\n",
        "    n_words += idx_from\n",
        "    embedding_mat = np.random.uniform(low=-random_uniform_level, high=random_uniform_level, size=(n_words, embedding_dim))\n",
        "    embedding_mat[0] = np.zeros(embedding_dim)\n",
        "    for word, idx in word2index.items():\n",
        "        if idx < idx_from:\n",
        "            continue\n",
        "        embedding_vec = embeddings.get(word)\n",
        "        if embedding_vec is not None:  # means we have this word's embedding\n",
        "            embedding_mat[idx] = embedding_vec\n",
        "    return embedding_mat\n",
        "\n",
        "\n",
        "# ========== keras utils ==========\n",
        "def att_process(candidates, att, activation='tanh'):\n",
        "    \"\"\"\n",
        "    Attention Process (functional API, can get weights at the same time)\n",
        "     - candidates: (*, maxlen, features)\n",
        "     - att: (*, att_dim)\n",
        "    \"\"\"\n",
        "    att_dim = K.int_shape(att)[-1]\n",
        "    candidates2 = layers.TimeDistributed(\n",
        "        layers.Dense(att_dim, activation=activation))(candidates)\n",
        "    dotted = layers.dot([candidates2, att], axes=(2, 1), normalize=True)\n",
        "    weights = layers.Activation('softmax')(dotted)  # (*, maxlen), sums up to 1\n",
        "    weighted = layers.dot([candidates, weights], axes=(1, 1))\n",
        "    return weighted, weights\n",
        "\n",
        "\n",
        "class UpdateMonitor(Callback):\n",
        "    \"\"\"monitor a model's training process:\n",
        "    monitor each layer's update rate (~1e-3 is good rate)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(UpdateMonitor, self).__init__()\n",
        "        self.weights = None\n",
        "\n",
        "#     @classmethod\n",
        "    def _get_updates(cls, old_weights, new_weights):\n",
        "        \"\"\"Calculate updates rate for layers' weights\n",
        "        Note: only calculate the first parameter of a layer\"\"\"\n",
        "        if not old_weights:\n",
        "            old_weights = new_weights\n",
        "        updates = []\n",
        "        for old_layerwise_weights, new_layerwise_weights in zip(old_weights, new_weights):\n",
        "            if len(old_layerwise_weights) == 0 or len(new_layerwise_weights) == 0:\n",
        "                updates.append(None)\n",
        "            else:\n",
        "                w1, w2 = old_layerwise_weights[0], new_layerwise_weights[0]  # only check the first weight of a layer\n",
        "                updates.append(norm(w2 - w1) / norm(w2))\n",
        "        return updates\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        # monitor update rates\n",
        "        new_weights = _get_weights(self.model)\n",
        "        updates = self._get_updates(old_weights=self.weights, new_weights=new_weights)\n",
        "        self.weights = new_weights  # update\n",
        "        updates_info = ', '.join('{:.4f}'.format(1e3 * update) if update else '-' for update in updates)\n",
        "        print('- updates: 1e-3 * [{}]'.format(updates_info))\n",
        "\n",
        "\n",
        "def _get_weights(model):\n",
        "    \"\"\"Get all layers' weights as a list of list:\n",
        "    [[l1_w1, l1_w2, ...], ... , [ln_w1, ln_w2, ...]]\"\"\"\n",
        "    weights = []\n",
        "    for layer in model.layers:\n",
        "        # if no weights, return value is []\n",
        "        weights.append(layer.get_weights())\n",
        "    return weights\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-cae749f45b45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# TODO: set your glove data path here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mglove_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/datasets/glove/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMj8rEYm7fPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _tvt_split(_seqs, _slabels, splits=(7, 2, 1)):\n",
        "    \"\"\"train/val/test split for one single domain\"\"\"\n",
        "    assert len(_seqs) == len(_slabels)\n",
        "    splits = np.asarray(splits)\n",
        "    splits = np.cumsum(splits / splits.sum())\n",
        "    # shuffle\n",
        "    indices = [range(len(_seqs))]\n",
        "    np.random.shuffle(indices)\n",
        "    _seqs = _seqs[indices]\n",
        "    _slabels = _slabels[indices]\n",
        "    # prepare data (balance data from all labels)\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = [], [], [], [], [], []\n",
        "    for slabel in sorted(np.unique(_slabels)):\n",
        "        seqs_ofs = _seqs[_slabels == slabel]\n",
        "        slabels_ofs = _slabels[_slabels == slabel]\n",
        "        # split\n",
        "        split_ats = np.asarray(splits * len(seqs_ofs), dtype=int)\n",
        "        X_train.extend(seqs_ofs[:split_ats[0]])\n",
        "        X_val.extend(seqs_ofs[split_ats[0]:split_ats[1]])\n",
        "        X_test.extend(seqs_ofs[split_ats[1]:])\n",
        "        y_train.extend(slabels_ofs[:split_ats[0]])\n",
        "        y_val.extend(slabels_ofs[split_ats[0]:split_ats[1]])\n",
        "        y_test.extend(slabels_ofs[split_ats[1]:])\n",
        "    X_train = np.asarray(X_train, dtype='int')\n",
        "    X_val = np.asarray(X_val, dtype='int')\n",
        "    X_test = np.asarray(X_test, dtype='int')\n",
        "    y_train = np.asarray(y_train, dtype='int')\n",
        "    y_val = np.asarray(y_val, dtype='int')\n",
        "    y_test = np.asarray(y_test, dtype='int')\n",
        "    print(' * X:', X_train.shape, X_val.shape, X_test.shape)\n",
        "    print(' * y:', y_train.shape, y_val.shape, y_test.shape)\n",
        "    return (X_train, X_val, X_test), (y_train, y_val, y_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQldPFmC79Kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_data():\n",
        "    \"\"\"data pre-processing\"\"\"\n",
        "\n",
        "    # load data\n",
        "    print('loading data: Multi-Domain Sentiment Dataset v2')\n",
        "    texts, s_labels, d_labels = load_mdsd(domains=DOMAINS)\n",
        "\n",
        "    # build vocabulary for words\n",
        "    print('building vocabulary')\n",
        "    texts_tokens = []\n",
        "    lens = []\n",
        "    for text in texts:\n",
        "        words = word_tokenize(text)\n",
        "        for idx, word in enumerate(words):\n",
        "            if word.isdigit():\n",
        "                words[idx] = '<NUM>'  # replace number token with <NUM>\n",
        "        texts_tokens.append(words)\n",
        "        lens.append(len(words))\n",
        "    maxlen = int(np.percentile(lens, 95))\n",
        "    print('maxlen:', maxlen)\n",
        "    counter = Counter()\n",
        "    for words in texts_tokens:\n",
        "        counter.update(words)\n",
        "    word2index = {'<PAD>': 0, '<UNK>': 1}\n",
        "    for idx, word_count in enumerate(counter.most_common(max_words)):\n",
        "        if word_count[1] >= min_count:  # min_count\n",
        "            word2index[word_count[0]] = idx + 2  # starting from 2, 0 used as <PAD>, 1 used as <OOV>\n",
        "    n_words = len(word2index)\n",
        "    print('n_words:', n_words)\n",
        "\n",
        "    # data encode\n",
        "    print('data encoding')\n",
        "    seqs = []\n",
        "    for words in texts_tokens:\n",
        "        seqs.append([word2index.get(word, 1) for word in words])\n",
        "    seqs_padded = pad_sequences(seqs, maxlen=maxlen, padding='post', truncating='post')\n",
        "    s_labels = np.asarray(s_labels, dtype=int)\n",
        "    d_labels = np.asarray(d_labels, dtype=int)\n",
        "\n",
        "    # domain & train/val/test split\n",
        "    print('labeled data: domain & train/val/test splitting')\n",
        "    X_train, ys_train, yd_train = [], [], []\n",
        "    X_val, ys_val, yd_val = [], [], []\n",
        "    X_test_byd, ys_test_byd, yd_test_byd = {}, {}, {}\n",
        "    for d_id, d_name in enumerate(DOMAINS):\n",
        "        print(d_name, 'splitting')\n",
        "        seqs_padded_ofd = seqs_padded[(d_labels == d_id) & (s_labels != -1)]\n",
        "        slabels_ofd = s_labels[(d_labels == d_id) & (s_labels != -1)]\n",
        "        print(' * all:', seqs_padded_ofd.shape, slabels_ofd.shape)\n",
        "        (X_train_ofd, X_val_ofd, X_test_ofd), (y_train_ofd, y_val_ofd, y_test_ofd) = _tvt_split(seqs_padded_ofd, slabels_ofd)\n",
        "        # train data (add this domain)\n",
        "        X_train.extend(X_train_ofd)\n",
        "        ys_train.extend(y_train_ofd)\n",
        "        yd_train.extend([d_id] * len(X_train_ofd))\n",
        "        # val data\n",
        "        X_val.extend(X_val_ofd)\n",
        "        ys_val.extend(y_val_ofd)\n",
        "        yd_val.extend([d_id] * len(X_val_ofd))\n",
        "        # test data\n",
        "        X_test_byd[d_id] = X_test_ofd\n",
        "        ys_test_byd[d_id] = to_categorical(y_test_ofd, num_classes=2)\n",
        "        yd_test_byd[d_id] = to_categorical([d_id] * len(X_test_ofd), num_classes=len(DOMAINS))\n",
        "    X_train = np.asarray(X_train, dtype='int')\n",
        "    ys_train = to_categorical(ys_train, num_classes=2)\n",
        "    yd_train = to_categorical(yd_train, num_classes=len(DOMAINS))\n",
        "    X_val = np.asarray(X_val, dtype='int')\n",
        "    ys_val = to_categorical(ys_val, num_classes=2)\n",
        "    yd_val = to_categorical(yd_val, num_classes=len(DOMAINS))\n",
        "    # combine test data from different domains\n",
        "    X_test = np.concatenate([X_test_byd[idx] for idx in range(len(DOMAINS))])\n",
        "    ys_test = np.concatenate([ys_test_byd[idx] for idx in range(len(DOMAINS))])\n",
        "    yd_test = np.concatenate([yd_test_byd[idx] for idx in range(len(DOMAINS))])\n",
        "\n",
        "    # shuffle train data\n",
        "    indices = list(range(len(X_train)))\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    ys_train = ys_train[indices]\n",
        "    yd_train = yd_train[indices]\n",
        "    print('combined labeled data:')\n",
        "    print('  - train:', X_train.shape, ys_train.shape, yd_train.shape)\n",
        "    print('  - val:', X_val.shape, ys_val.shape, yd_val.shape)\n",
        "    print('  - test:', X_test.shape, ys_test.shape, yd_test.shape)\n",
        "    for d_id, d_name in enumerate(DOMAINS):\n",
        "        print('  - test for {}:'.format(d_name[:3]), X_test_byd[d_id].shape, ys_test_byd[d_id].shape, yd_test_byd[d_id].shape)\n",
        "\n",
        "    # embeddings\n",
        "    print('loading word embeddings from glove')\n",
        "    embeddings = load_glove(embedding_dim=embed_dim, desired=word2index.keys(), corpus_size=glove_corpus)\n",
        "    print('processing embedding matrix')\n",
        "    embedding_mat = get_embedding_mat(embeddings, word2index, embed_dim, idx_from=2)\n",
        "    wv_weights = [embedding_mat]\n",
        "\n",
        "    # inject data into SharedData for other functions\n",
        "    maxlen = maxlen\n",
        "    n_words = n_words\n",
        "    word2index = word2index\n",
        "    X_train, ys_train, yd_train = X_train, ys_train, yd_train\n",
        "    X_val, ys_val, yd_val = X_val, ys_val, yd_val\n",
        "    X_test, ys_test, yd_test = X_test, ys_test, yd_test\n",
        "    X_test_byd, ys_test_byd, yd_test_byd = X_test_byd, ys_test_byd, yd_test_byd\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IBGqppy8vZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "\n",
        "    # load embeddings\n",
        "    weights = wv_weights\n",
        "\n",
        "    # the model\n",
        "    print('\\nbuilding the model')\n",
        "    inputs = layers.Input(shape=(maxlen,))\n",
        "    embeddings = layers.Embedding(\n",
        "        input_dim= n_words,\n",
        "        output_dim= embed_dim,\n",
        "        input_length= maxlen,\n",
        "        weights=weights)(inputs)\n",
        "    embeddings = layers.SpatialDropout1D(rate=embed_dropout)(embeddings)\n",
        "\n",
        "    # domain part\n",
        "    d_repr = layers.Bidirectional(RNN(\n",
        "        units=rnn_dim,\n",
        "        return_sequences=False))(embeddings)\n",
        "    d_repr = layers.Dense(hidden_dim, activation=activation)(d_repr)\n",
        "    d_repr = layers.Dropout(fc_dropout)(d_repr)\n",
        "    d_pred = layers.Dense(len(DOMAINS), activation='softmax', name='d_pred')(d_repr)\n",
        "\n",
        "    # senti part\n",
        "    # use domain representation as attention\n",
        "    episodes = layers.Bidirectional(RNN(\n",
        "        units=rnn_dim,\n",
        "        return_sequences=True))(embeddings)\n",
        "    selected, _ = att_process(candidates=episodes, att=d_repr)\n",
        "    s_repr = layers.Dense(hidden_dim, activation=activation)(selected)\n",
        "    s_repr = layers.Dropout(fc_dropout)(s_repr)\n",
        "    s_pred = layers.Dense(2, activation='softmax', name='s_pred')(s_repr)\n",
        "\n",
        "    # model\n",
        "    model = Model(\n",
        "        inputs=inputs,\n",
        "        outputs=[s_pred, d_pred])\n",
        "    model.compile(optimizer=optimizer, metrics=['acc'], loss={\n",
        "        's_pred': 'categorical_crossentropy',\n",
        "        'd_pred': 'categorical_crossentropy'\n",
        "    }, loss_weights={\n",
        "        's_pred': 1,\n",
        "        'd_pred': coef\n",
        "    })\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_and_test(model):\n",
        "\n",
        "    # training\n",
        "    updater = UpdateMonitor()\n",
        "    reducer = callbacks.ReduceLROnPlateau(factor=lr_factor, patience=lr_patience, verbose=1)\n",
        "    stopper = callbacks.EarlyStopping(patience=stop_patience, verbose=1)\n",
        "    cbks = [updater, reducer, stopper]\n",
        "    print('\\ntraining model')\n",
        "    model.fit(\n",
        "        X_train,\n",
        "        [ys_train, yd_train],\n",
        "        validation_data=(X_val, [ys_val, yd_val]),\n",
        "        shuffle=True, batch_size=batch_size, epochs=epochs, verbose=2,\n",
        "        callbacks=cbks)\n",
        "\n",
        "    # evaluation\n",
        "    print('\\nTest evaluation:')\n",
        "    for d_id, d_name in enumerate(DOMAINS):\n",
        "        scores = model.evaluate(\n",
        "            X_test_byd[d_id],\n",
        "            [ys_test_byd[d_id], yd_test_byd[d_id]],\n",
        "            batch_size=batch_size, verbose=0)\n",
        "        print('{} acc: {:.4f}'.format(d_name[:3], scores[-2]))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # data process\n",
        "    make_data()\n",
        "\n",
        "    # build & compile model\n",
        "    model = get_model()\n",
        "\n",
        "    # train and test\n",
        "    train_and_test(model)\n",
        "\n",
        "    print('\\nprocess finished ~~~')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}