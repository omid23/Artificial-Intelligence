{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper source -- https://www.sciencedirect.com/science/article/abs/pii/S0950705118302144",
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8JHMrLBu9FQO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras import layers, callbacks\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import re\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from keras import layers\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(2333)\n",
    "tf.set_random_seed(2333)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TsbAEiZl9hdT"
   },
   "outputs": [],
   "source": [
    "glove_path = os.path.expanduser('glove.6B')\n",
    "\n",
    "# domains\n",
    "DOMAINS = ('books', 'dvd', 'electronics', 'kitchen')\n",
    "\n",
    "\n",
    "\"\"\"global data\"\"\"\n",
    "coef = 0.04  # domain loss coefficient in total loss\n",
    "\n",
    "# model params\n",
    "embed_dim = 300  # word embedding dimension\n",
    "rnn_dim = 300  # rnn state dimension\n",
    "hidden_dim = 100  # hidden state dimension\n",
    "embed_dropout = 0.2  # dropout rate for word embedding layer\n",
    "fc_dropout = 0.2  # dropout rate for fully connected layer\n",
    "batch_size = 64  # training batch size\n",
    "epochs = 1  # maximal training epochs\n",
    "activation = 'relu'  # activation function of fully connected layer\n",
    "optimizer = 'adam'  # optimizer method for training\n",
    "RNN = layers.LSTM  # RNN type, can be SimpleRNN/GRU/LSTM/...\n",
    "\n",
    "# train params\n",
    "lr_factor = 0.1  # reduce factor of learning rate for training plateau\n",
    "lr_patience = 3  # reduce learning rate when val loss has stopped improving\n",
    "stop_patience = 5  # stop training when val loss has stopped improving\n",
    "\n",
    "# data params\n",
    "glove_corpus = 6  # glove corpus size, can be 6B/42B/840B\n",
    "min_count = 1  # minimal word frequency cutoff\n",
    "max_words = None  # maximal words kept (None means all kept)\n",
    "n_words = None\n",
    "maxlen = None\n",
    "word2index = None\n",
    "wv_weights = None  # word vectors weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader for MDSD Dataset (Multi-Domain Sentiment Dataset )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-PXSwEdwSt3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_mdsd(domains, n_labeled=None):\n",
    "    texts = []\n",
    "    s_labels = []\n",
    "    d_labels = []\n",
    "    sentiments = ('positive', 'negative')\n",
    "    for d_id, d_name in enumerate(domains):\n",
    "        for s_id, s_name in zip((1, 0, -1), sentiments):\n",
    "            fpath = os.path.join('', 'datasets', d_name + '_' + s_name + '.review')\n",
    "            print(' - loading', d_name, s_name, end='')\n",
    "            count = 0\n",
    "            text = ''\n",
    "            in_review_text = False\n",
    "            with open(fpath, encoding='utf8', errors='ignore') as fr:\n",
    "                for line in fr:\n",
    "                    if '<review_text>' in line:\n",
    "                        text = ''\n",
    "                        in_review_text = True\n",
    "                        continue\n",
    "                    if '</review_text>' in line:\n",
    "                        in_review_text = False\n",
    "                        text = text.lower().replace('\\n', ' ').strip()\n",
    "                        text = re.sub(r'&[a-z]+;', '', text)\n",
    "                        text = re.sub(r'\\s+', ' ', text)\n",
    "                        texts.append(text)\n",
    "                        s_labels.append(s_id)\n",
    "                        d_labels.append(d_id)\n",
    "                        count += 1\n",
    "                    if in_review_text:\n",
    "                        text += line\n",
    "                    # labeled cutoff\n",
    "                    if (s_id >= 0) and n_labeled and (count == n_labeled):\n",
    "                        break\n",
    "            print(': %d texts' % count)\n",
    "    print('data loaded')\n",
    "    s_labels = np.asarray(s_labels, dtype='int')\n",
    "    d_labels = np.asarray(d_labels, dtype='int')\n",
    "    print(' - texts:', len(texts))\n",
    "    print(' - s_labels:', len(s_labels))\n",
    "    print(' - d_labels:', len(d_labels))\n",
    "\n",
    "    return texts, s_labels, d_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "bT-z5KWR1Mt7",
    "outputId": "6484a567-7226-4e7e-b613-32d31eef394f"
   },
   "outputs": [],
   "source": [
    "assert os.path.exists(glove_path)\n",
    "\n",
    "\n",
    "def load_glove(path=glove_path, embedding_dim=300, corpus_size=6, desired=None, verbose=False):\n",
    "    \"\"\"Load glove embeddings from original txt file\n",
    "    \"\"\"\n",
    "    if embedding_dim != 300:\n",
    "        assert embedding_dim in (50, 100, 200), 'embedding dim must be one of 50/100/200 if not 300'\n",
    "        fpath = os.path.join(path, 'glove.6B.{}d.txt'.format(embedding_dim))\n",
    "    else:\n",
    "        assert corpus_size in (6, 42, 840), 'corpus type must be one of 6B/42B/840B'\n",
    "        fpath = os.path.join(path, 'glove.{}B.300d.txt'.format(corpus_size))\n",
    "    word2vec = {}\n",
    "    print('loading glove from', fpath)\n",
    "    f = open(fpath, 'r', encoding='utf8', errors='ignore')\n",
    "    for line in f if verbose else f:\n",
    "        values = line.split()\n",
    "        word = values[0]  # the word\n",
    "        if not desired or word in desired:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            word2vec[word] = coefs\n",
    "    f.close()\n",
    "    print('glove info: {} words, {} dims'.format(len(word2vec), embedding_dim))\n",
    "    return word2vec\n",
    "\n",
    "\n",
    "def get_embedding_mat(embeddings, word2index, embedding_dim, random_uniform_level=0.01, idx_from=2):\n",
    "    \"\"\"Use embeddings and word2index to get embedding-mat (for input layer)\n",
    "    idx_from=2, usually, 0 for <PAD>, 1 for <OOV>\n",
    "    \"\"\"\n",
    "    # embedding_mat = np.zeros((n_words, embedding_dim))\n",
    "    n_words = len(word2index)\n",
    "    for idx in range(0, idx_from):\n",
    "        if idx in word2index.values():\n",
    "            n_words -= 1\n",
    "    n_words += idx_from\n",
    "    embedding_mat = np.random.uniform(low=-random_uniform_level, high=random_uniform_level, size=(n_words, embedding_dim))\n",
    "    embedding_mat[0] = np.zeros(embedding_dim)\n",
    "    for word, idx in word2index.items():\n",
    "        if idx < idx_from:\n",
    "            continue\n",
    "        embedding_vec = embeddings.get(word)\n",
    "        if embedding_vec is not None:  # means we have this word's embedding\n",
    "            embedding_mat[idx] = embedding_vec\n",
    "    return embedding_mat\n",
    "\n",
    "\n",
    "# ========== keras utils ==========\n",
    "def att_process(candidates, att, activation='tanh'):\n",
    "    \"\"\"\n",
    "    Attention Process (functional API, can get weights at the same time)\n",
    "     - candidates: (*, maxlen, features)\n",
    "     - att: (*, att_dim)\n",
    "    \"\"\"\n",
    "    att_dim = K.int_shape(att)[-1]\n",
    "    candidates2 = layers.TimeDistributed(\n",
    "        layers.Dense(att_dim, activation=activation))(candidates)\n",
    "    dotted = layers.dot([candidates2, att], axes=(2, 1), normalize=True)\n",
    "    weights = layers.Activation('softmax')(dotted)  # (*, maxlen), sums up to 1\n",
    "    weighted = layers.dot([candidates, weights], axes=(1, 1))\n",
    "    return weighted, weights\n",
    "\n",
    "\n",
    "class UpdateMonitor(Callback):\n",
    "    \"\"\"monitor a model's training process:\n",
    "    monitor each layer's update rate (~1e-3 is good rate)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(UpdateMonitor, self).__init__()\n",
    "        self.weights = None\n",
    "\n",
    "#     @classmethod\n",
    "    def _get_updates(cls, old_weights, new_weights):\n",
    "        \"\"\"Calculate updates rate for layers' weights\n",
    "        Note: only calculate the first parameter of a layer\"\"\"\n",
    "        if not old_weights:\n",
    "            old_weights = new_weights\n",
    "        updates = []\n",
    "        for old_layerwise_weights, new_layerwise_weights in zip(old_weights, new_weights):\n",
    "            if len(old_layerwise_weights) == 0 or len(new_layerwise_weights) == 0:\n",
    "                updates.append(None)\n",
    "            else:\n",
    "                w1, w2 = old_layerwise_weights[0], new_layerwise_weights[0]  # only check the first weight of a layer\n",
    "                updates.append(norm(w2 - w1) / norm(w2))\n",
    "        return updates\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # monitor update rates\n",
    "        new_weights = _get_weights(self.model)\n",
    "        updates = self._get_updates(old_weights=self.weights, new_weights=new_weights)\n",
    "        self.weights = new_weights  # update\n",
    "        updates_info = ', '.join('{:.4f}'.format(1e3 * update) if update else '-' for update in updates)\n",
    "        print('- updates: 1e-3 * [{}]'.format(updates_info))\n",
    "\n",
    "\n",
    "def _get_weights(model):\n",
    "    \"\"\"Get all layers' weights as a list of list:\n",
    "    [[l1_w1, l1_w2, ...], ... , [ln_w1, ln_w2, ...]]\"\"\"\n",
    "    weights = []\n",
    "    for layer in model.layers:\n",
    "        # if no weights, return value is []\n",
    "        weights.append(layer.get_weights())\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/val/test split for one single domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMj8rEYm7fPg"
   },
   "outputs": [],
   "source": [
    "def _tvt_split(_seqs, _slabels, splits=(7, 2, 1)):\n",
    "    assert len(_seqs) == len(_slabels)\n",
    "    splits = np.asarray(splits)\n",
    "    splits = np.cumsum(splits / splits.sum())\n",
    "    # shuffle\n",
    "    indices = [range(len(_seqs))]\n",
    "    np.random.shuffle(indices)\n",
    "    _seqs = _seqs[indices]\n",
    "    _slabels = _slabels[indices]\n",
    "    # prepare data (balance data from all labels)\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = [], [], [], [], [], []\n",
    "    for slabel in sorted(np.unique(_slabels)):\n",
    "        seqs_ofs = _seqs[_slabels == slabel]\n",
    "        slabels_ofs = _slabels[_slabels == slabel]\n",
    "        # split\n",
    "        split_ats = np.asarray(splits * len(seqs_ofs), dtype=int)\n",
    "        X_train.extend(seqs_ofs[:split_ats[0]])\n",
    "        X_val.extend(seqs_ofs[split_ats[0]:split_ats[1]])\n",
    "        X_test.extend(seqs_ofs[split_ats[1]:])\n",
    "        y_train.extend(slabels_ofs[:split_ats[0]])\n",
    "        y_val.extend(slabels_ofs[split_ats[0]:split_ats[1]])\n",
    "        y_test.extend(slabels_ofs[split_ats[1]:])\n",
    "    X_train = np.asarray(X_train, dtype='int')\n",
    "    X_val = np.asarray(X_val, dtype='int')\n",
    "    X_test = np.asarray(X_test, dtype='int')\n",
    "    y_train = np.asarray(y_train, dtype='int')\n",
    "    y_val = np.asarray(y_val, dtype='int')\n",
    "    y_test = np.asarray(y_test, dtype='int')\n",
    "    print(' * X:', X_train.shape, X_val.shape, X_test.shape)\n",
    "    print(' * y:', y_train.shape, y_val.shape, y_test.shape)\n",
    "    return (X_train[:100], X_val[:100], X_test[:100]), (y_train[:100], y_val[:100], y_test[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQldPFmC79Kc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data: Multi-Domain Sentiment Dataset v2\n",
      " - loading books positive: 1000 texts\n",
      " - loading books negative: 1000 texts\n",
      " - loading dvd positive: 1000 texts\n",
      " - loading dvd negative: 1000 texts\n",
      " - loading electronics positive: 1000 texts\n",
      " - loading electronics negative: 1000 texts\n",
      " - loading kitchen positive: 1000 texts\n",
      " - loading kitchen negative: 1000 texts\n",
      "data loaded\n",
      " - texts: 8000\n",
      " - s_labels: 8000\n",
      " - d_labels: 8000\n",
      "building vocabulary\n",
      "maxlen: 461\n",
      "n_words: 45577\n",
      "data encoding\n",
      "labeled data: domain & train/val/test splitting\n",
      "books splitting\n",
      " * all: (2000, 461) (2000,)\n",
      " * X: (1400, 461) (398, 461) (202, 461)\n",
      " * y: (1400,) (398,) (202,)\n",
      "dvd splitting\n",
      " * all: (2000, 461) (2000,)\n",
      " * X: (1400, 461) (398, 461) (202, 461)\n",
      " * y: (1400,) (398,) (202,)\n",
      "electronics splitting\n",
      " * all: (2000, 461) (2000,)\n",
      " * X: (1400, 461) (398, 461) (202, 461)\n",
      " * y: (1400,) (398,) (202,)\n",
      "kitchen splitting\n",
      " * all: (2000, 461) (2000,)\n",
      " * X: (1400, 461) (398, 461) (202, 461)\n",
      " * y: (1400,) (398,) (202,)\n",
      "combined labeled data:\n",
      "  - train: (400, 461) (400, 2) (400, 4)\n",
      "  - val: (400, 461) (400, 2) (400, 4)\n",
      "  - test: (400, 461) (400, 2) (400, 4)\n",
      "  - test for boo: (100, 461) (100, 2) (100, 4)\n",
      "  - test for dvd: (100, 461) (100, 2) (100, 4)\n",
      "  - test for ele: (100, 461) (100, 2) (100, 4)\n",
      "  - test for kit: (100, 461) (100, 2) (100, 4)\n",
      "loading word embeddings from glove\n",
      "loading glove from glove.6B/glove.6B.300d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:8: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:9: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove info: 35110 words, 300 dims\n",
      "processing embedding matrix\n"
     ]
    }
   ],
   "source": [
    "def make_data():\n",
    "    # load data\n",
    "    print('loading data: Multi-Domain Sentiment Dataset v2')\n",
    "    texts, s_labels, d_labels = load_mdsd(domains=DOMAINS)\n",
    "\n",
    "    # build vocabulary for words\n",
    "    print('building vocabulary')\n",
    "    texts_tokens = []\n",
    "    lens = []\n",
    "    for text in texts:\n",
    "        words = word_tokenize(text)\n",
    "        for idx, word in enumerate(words):\n",
    "            if word.isdigit():\n",
    "                words[idx] = '<NUM>'  # replace number token with <NUM>\n",
    "        texts_tokens.append(words)\n",
    "        lens.append(len(words))\n",
    "    maxlen = int(np.percentile(lens, 95))\n",
    "    print('maxlen:', maxlen)\n",
    "    counter = Counter()\n",
    "    for words in texts_tokens:\n",
    "        counter.update(words)\n",
    "    word2index = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for idx, word_count in enumerate(counter.most_common(max_words)):\n",
    "        if word_count[1] >= min_count:  # min_count\n",
    "            word2index[word_count[0]] = idx + 2  # starting from 2, 0 used as <PAD>, 1 used as <OOV>\n",
    "    n_words = len(word2index)\n",
    "    print('n_words:', n_words)\n",
    "\n",
    "    # data encode\n",
    "    print('data encoding')\n",
    "    seqs = []\n",
    "    for words in texts_tokens:\n",
    "        seqs.append([word2index.get(word, 1) for word in words])\n",
    "    seqs_padded = pad_sequences(seqs, maxlen=maxlen, padding='post', truncating='post')\n",
    "    s_labels = np.asarray(s_labels, dtype=int)\n",
    "    d_labels = np.asarray(d_labels, dtype=int)\n",
    "\n",
    "    # domain & train/val/test split\n",
    "    print('labeled data: domain & train/val/test splitting')\n",
    "    X_train, ys_train, yd_train = [], [], []\n",
    "    X_val, ys_val, yd_val = [], [], []\n",
    "    X_test_byd, ys_test_byd, yd_test_byd = {}, {}, {}\n",
    "    for d_id, d_name in enumerate(DOMAINS):\n",
    "        print(d_name, 'splitting')\n",
    "        seqs_padded_ofd = seqs_padded[(d_labels == d_id) & (s_labels != -1)]\n",
    "        slabels_ofd = s_labels[(d_labels == d_id) & (s_labels != -1)]\n",
    "        print(' * all:', seqs_padded_ofd.shape, slabels_ofd.shape)\n",
    "        (X_train_ofd, X_val_ofd, X_test_ofd), (y_train_ofd, y_val_ofd, y_test_ofd) = _tvt_split(seqs_padded_ofd, slabels_ofd)\n",
    "        # train data (add this domain)\n",
    "        X_train.extend(X_train_ofd)\n",
    "        ys_train.extend(y_train_ofd)\n",
    "        yd_train.extend([d_id] * len(X_train_ofd))\n",
    "        # val data\n",
    "        X_val.extend(X_val_ofd)\n",
    "        ys_val.extend(y_val_ofd)\n",
    "        yd_val.extend([d_id] * len(X_val_ofd))\n",
    "        # test data\n",
    "        X_test_byd[d_id] = X_test_ofd\n",
    "        ys_test_byd[d_id] = to_categorical(y_test_ofd, num_classes=2)\n",
    "        yd_test_byd[d_id] = to_categorical([d_id] * len(X_test_ofd), num_classes=len(DOMAINS))\n",
    "    X_train = np.asarray(X_train, dtype='int')\n",
    "    ys_train = to_categorical(ys_train, num_classes=2)\n",
    "    yd_train = to_categorical(yd_train, num_classes=len(DOMAINS))\n",
    "    X_val = np.asarray(X_val, dtype='int')\n",
    "    ys_val = to_categorical(ys_val, num_classes=2)\n",
    "    yd_val = to_categorical(yd_val, num_classes=len(DOMAINS))\n",
    "    # combine test data from different domains\n",
    "    X_test = np.concatenate([X_test_byd[idx] for idx in range(len(DOMAINS))])\n",
    "    ys_test = np.concatenate([ys_test_byd[idx] for idx in range(len(DOMAINS))])\n",
    "    yd_test = np.concatenate([yd_test_byd[idx] for idx in range(len(DOMAINS))])\n",
    "\n",
    "    # shuffle train data\n",
    "    indices = list(range(len(X_train)))\n",
    "    np.random.shuffle(indices)\n",
    "    X_train = X_train[indices]\n",
    "    ys_train = ys_train[indices]\n",
    "    yd_train = yd_train[indices]\n",
    "    print('combined labeled data:')\n",
    "    print('  - train:', X_train.shape, ys_train.shape, yd_train.shape)\n",
    "    print('  - val:', X_val.shape, ys_val.shape, yd_val.shape)\n",
    "    print('  - test:', X_test.shape, ys_test.shape, yd_test.shape)\n",
    "    for d_id, d_name in enumerate(DOMAINS):\n",
    "        print('  - test for {}:'.format(d_name[:3]), X_test_byd[d_id].shape, ys_test_byd[d_id].shape, yd_test_byd[d_id].shape)\n",
    "\n",
    "    # embeddings\n",
    "    print('loading word embeddings from glove')\n",
    "    embeddings = load_glove(embedding_dim=embed_dim, desired=word2index.keys(), corpus_size=glove_corpus)\n",
    "    print('processing embedding matrix')\n",
    "    embedding_mat = get_embedding_mat(embeddings, word2index, embed_dim, idx_from=2)\n",
    "    wv_weights = [embedding_mat]\n",
    "\n",
    "    # inject data into SharedData for other functions\n",
    "    maxlen = maxlen\n",
    "    n_words = n_words\n",
    "    word2index = word2index\n",
    "    X_train, ys_train, yd_train = X_train, ys_train, yd_train\n",
    "    X_val, ys_val, yd_val = X_val, ys_val, yd_val\n",
    "    X_test, ys_test, yd_test = X_test, ys_test, yd_test\n",
    "    X_test_byd, ys_test_byd, yd_test_byd = X_test_byd, ys_test_byd, yd_test_byd\n",
    "    return wv_weights, maxlen, n_words, word2index, X_train, X_val, ys_val, yd_val, X_test, ys_test, yd_test, X_test_byd, ys_test_byd, yd_test_byd, ys_train, yd_train\n",
    "\n",
    "wv_weights, maxlen, n_words, word2index, X_train, X_val, ys_val, yd_val, X_test, ys_test, yd_test, X_test_byd, ys_test_byd, yd_test_byd, ys_train, yd_train = make_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3IBGqppy8vZn"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "    # load embeddings\n",
    "    weights = wv_weights\n",
    "\n",
    "    # the model\n",
    "    print('\\nbuilding the model')\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embeddings = layers.Embedding(\n",
    "        input_dim= n_words,\n",
    "        output_dim= embed_dim,\n",
    "        input_length= maxlen,\n",
    "        weights=weights)(inputs)\n",
    "    embeddings = layers.SpatialDropout1D(rate=embed_dropout)(embeddings)\n",
    "\n",
    "    # domain part\n",
    "    d_repr = layers.Bidirectional(RNN(\n",
    "        units=rnn_dim,\n",
    "        return_sequences=False))(embeddings)\n",
    "    d_repr = layers.Dense(hidden_dim, activation=activation)(d_repr)\n",
    "    d_repr = layers.Dropout(fc_dropout)(d_repr)\n",
    "    d_pred = layers.Dense(len(DOMAINS), activation='softmax', name='d_pred')(d_repr)\n",
    "\n",
    "    # senti part\n",
    "    # use domain representation as attention\n",
    "    episodes = layers.Bidirectional(RNN(\n",
    "        units=rnn_dim,\n",
    "        return_sequences=True))(embeddings)\n",
    "    selected, _ = att_process(candidates=episodes, att=d_repr)\n",
    "    s_repr = layers.Dense(hidden_dim, activation=activation)(selected)\n",
    "    s_repr = layers.Dropout(fc_dropout)(s_repr)\n",
    "    s_pred = layers.Dense(2, activation='softmax', name='s_pred')(s_repr)\n",
    "\n",
    "    # model\n",
    "    model = Model(\n",
    "        inputs=inputs,\n",
    "        outputs=[s_pred, d_pred])\n",
    "    model.compile(optimizer=optimizer, metrics=['acc'], loss={\n",
    "        's_pred': 'categorical_crossentropy',\n",
    "        'd_pred': 'categorical_crossentropy'\n",
    "    }, loss_weights={\n",
    "        's_pred': 1,\n",
    "        'd_pred': coef\n",
    "    })\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "building the model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 461)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 461, 300)     13673100    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 461, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 600)          1442400     spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 461, 600)     1442400     spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          60100       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 461, 100)     60100       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 461)          0           time_distributed_1[0][0]         \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 461)          0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 600)          0           bidirectional_2[0][0]            \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          60100       dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "s_pred (Dense)                  (None, 2)            202         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d_pred (Dense)                  (None, 4)            404         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 16,738,806\n",
      "Trainable params: 16,738,806\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "training model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 400 samples, validate on 400 samples\n",
      "Epoch 1/1\n",
      " - 130s - loss: 0.4095 - s_pred_loss: 0.3530 - d_pred_loss: 1.4123 - s_pred_acc: 0.9125 - d_pred_acc: 0.2300 - val_loss: 0.0554 - val_s_pred_loss: 6.2022e-05 - val_d_pred_loss: 1.3824 - val_s_pred_acc: 1.0000 - val_d_pred_acc: 0.2825\n",
      "- updates: 1e-3 * [-, -, -, -, -, -, -, -, -, -, -, -, -, -, -]\n",
      "\n",
      "Test evaluation:\n",
      "boo acc: 1.0000\n",
      "dvd acc: 1.0000\n",
      "ele acc: 1.0000\n",
      "kit acc: 1.0000\n",
      "\n",
      "process finished ~~~\n"
     ]
    }
   ],
   "source": [
    "def train_and_test(model):\n",
    "\n",
    "    # training\n",
    "    updater = UpdateMonitor()\n",
    "    reducer = callbacks.ReduceLROnPlateau(factor=lr_factor, patience=lr_patience, verbose=1)\n",
    "    stopper = callbacks.EarlyStopping(patience=stop_patience, verbose=1)\n",
    "    cbks = [updater, reducer, stopper]\n",
    "    print('\\ntraining model')\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        [ys_train, yd_train],\n",
    "        validation_data=(X_val, [ys_val, yd_val]),\n",
    "        shuffle=True, batch_size=batch_size, epochs=epochs, verbose=2,\n",
    "        callbacks=cbks)\n",
    "\n",
    "    # evaluation\n",
    "    print('\\nTest evaluation:')\n",
    "    for d_id, d_name in enumerate(DOMAINS):\n",
    "        scores = model.evaluate(\n",
    "            X_test_byd[d_id],\n",
    "            [ys_test_byd[d_id], yd_test_byd[d_id]],\n",
    "            batch_size=batch_size, verbose=0)\n",
    "        print('{} acc: {:.4f}'.format(d_name[:3], scores[-2]))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # data process\n",
    "\n",
    "\n",
    "    # build & compile model\n",
    "    model = get_model()\n",
    "\n",
    "    # train and test\n",
    "    train_and_test(model)\n",
    "\n",
    "    print('\\nprocess finished ~~~')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DAM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
